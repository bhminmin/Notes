### Lecture 2
主要讲了核密度估计(KDE/Parzen窗)和最近邻分类(K-NN)。不假设分布形式，直接用数据估计$p(x)$或决策边界
#### 直方图
* 把空间分成格子，统计每个格子里的样本数估计密度
* 缺点：在高维空间效果差，依赖格子位置
---
#### Parzen窗(KDE)
* 用平滑的核函数（如高斯核）代替硬边界的直方图
* 带宽$h$控制平滑程度：
  * $h$小 -> 更细节（可能过拟合），更贴近数据
  * $h$大 -> 更平滑（可能欠拟合）
* 带宽选择：
  * 启发式公式（依赖样本方差，维度）
  * 交叉验证（Leave-One-Out）最大化似然
---
#### K-Nearest Neighbor(K-NN)
* 思路：给定一个样本$x$,找到它的$k$个最近邻
* 根据邻居的类别投票（或加权）预测
* 也可以看作是一种**变窗口密度估计**：
  * 固定$k$,局部体积$V_k$随数据密度变化而变动
  * 类条件估计密度${\hat{p}(x|\omega)}\approx\frac{k_m}{n_mV_k}$
  * 分类时，等价于比较各类在邻域中的票数$k_m$
* 与KDE对于：
  * KDE固定带宽$h$,k-NN固定邻居数$k$.
  * k-NN的窗口大小会局部密度自适应变化
* 性质：
  * $k$小 -> 决策边界更复杂
  * $k$大 -> 决策边界更平滑
  * 当$k$ -> $\infty$ 且样本无限时，错误率 -> $\leq$ 2$\times$ Bayes error
---
#### 两者的关系
* 同一视角：**密度估计$\rightarrow$ Bayes分类**
  * KDE：固定窗口大小$h$,样本点数随位置变化
  * K-NN：固定样本数$k$，窗口体积随位置变化
* 两者都在做局部统计，然后通过Bayes公式得到分类器
* 都可以通过交叉验证或留一法来选择最优参数